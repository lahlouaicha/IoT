{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVMFehG_XR3o"
   },
   "source": [
    "# 7. Hypertuning and Aggregating\n",
    "Ce notebook a pour but d'améliorer la prédiction de la localisation des devices.  \n",
    "\n",
    "\n",
    "* La première étape consiste à réaliser une <u>*correction des bases outliers*</u>.  \n",
    "* La deuxième étape consiste à <u>*retirer des devices difficiles à prédire*</u> des données d'entrainement. \n",
    "* La troisième étape repose sur une <u>*hyperparameter optimization*</u> des meilleurs modèles retenus.\n",
    "* La dernière étape réalise une <u>*méthode d'aggregating*</u> des modèles tunés \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUM-_Dz4Wx5G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import des librairies de Nicolas \n",
    "from IpyTools import *\n",
    "from IotTools import *\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "# Import des modèles à utiliser\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjYENpUfWx5V"
   },
   "outputs": [],
   "source": [
    "df_mess_train = pd.read_csv('mess_train_list.csv')\n",
    "df_mess_test = pd.read_csv('mess_test_list.csv')\n",
    "pos_train = pd.read_csv('pos_train_list.csv')\n",
    "listOfBs = np.union1d(df_mess_train.bsid.unique(),df_mess_test.bsid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vU2vLtpYWx5X"
   },
   "outputs": [],
   "source": [
    "X_train = df_mess_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHBgQfc8Wx5Y"
   },
   "source": [
    "## Fonction de protection réécrivant toutes les bases en dehors d'un certain cadre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIOXAk-cWx5Z",
    "outputId": "69019c8e-b266-416d-c7bb-5555e2926440"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons 27 bases outliers\n"
     ]
    }
   ],
   "source": [
    "X_train=Correct_Bases(df_mess_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqJ23iPwWx5c"
   },
   "source": [
    "## Suppression des devices difficiles à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcinzL6HWx5d",
    "outputId": "bd56756b-0307-4412-ecdb-43426fffc59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35668, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train[~X_train.did.isin([476598,476896,476256,476513,476889,476248,473288,476327,476836])]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQmCeNy2Wx5e"
   },
   "source": [
    "## Création de la matrice des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MqxTV0GbNul",
    "outputId": "8f6dfc69-97e9-46d2-a9e2-509a25af9334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4876, 3), (4876, 273))"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat, id_list=feat_mat_const(X_train, listOfBs)\n",
    "\n",
    "y_full = ground_truth_const(X_train, pos_train, id_list)\n",
    "y_full.shape,df_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yneZdsWQbNul"
   },
   "source": [
    "## Tuning des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqSSvI6tbNum"
   },
   "source": [
    "Afin d'améliorer les modèles retenus, une optimisation des hyperparamètres en utilisant un GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TsihIHUbNum"
   },
   "outputs": [],
   "source": [
    "def evaluation_model(model_lat,model_long, cv, df_feat,y_full) :\n",
    "    '''Return the score used to evaluate the model'''\n",
    "    y_pred_long = cross_val_predict(model_long, df_feat, y_full.lng, cv=cv, n_jobs=-1)\n",
    "    y_pred_lat = cross_val_predict(model_lat, df_feat, y_full.lat, cv=cv,n_jobs=-1)\n",
    "    err_vec = Eval_geoloc(y_full.lat , y_full.lng, y_pred_lat, y_pred_long)\n",
    "    score = np.percentile(err_vec, 80)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_Gob65SbNum"
   },
   "outputs": [],
   "source": [
    "def gridsearch_multiple_models(df_feat,cv,param,model):\n",
    "    '''Function that returns the best model for the longtitude and latitude'''\n",
    "    model_long = GridSearchCV(model, param,cv=cv,n_jobs=-1)\n",
    "    model_long.fit(df_feat, y_full.lng)\n",
    "    \n",
    "    model_lat = GridSearchCV(model, param,cv=cv,n_jobs=-1)\n",
    "    model_lat.fit(df_feat, y_full.lat)\n",
    "    \n",
    "    return model_lat, model_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_udMXVgbNum"
   },
   "outputs": [],
   "source": [
    "data = {'Model': [],  'Score_pre_tuning' : [], 'Score_post_tuning' : [], 'Hyperparams_lat' : [],  'Hyperparams_long' : [] } \n",
    "\n",
    "def dataframe_summary(name_model,pre_score,post_score,params_long,params_lat,data):\n",
    "    '''Function that returns a Pandas DataFrame and the dictionnary used to make this DF\n",
    "    This DataFrame gives information about the score of a model and its hyperparameters'''\n",
    "    data[\"Model\"].append(name_model)\n",
    "    data[\"Score_pre_tuning\"].append(pre_score)\n",
    "    data[\"Score_post_tuning\"].append(post_score)\n",
    "    data['Hyperparams_lat'].append(params_lat)\n",
    "    data['Hyperparams_long'].append(params_long) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9-yc2mDbNun"
   },
   "outputs": [],
   "source": [
    "models_description = [\n",
    "    {\"Model_name\" : \"XGBoost Regressor\", \"Model\" : xgb.XGBRegressor() ,\"params\" : {'max_depth':[4,5,6],'n_estimators' : [50,100,200],'learning_rate' : [0.1,0.2],'booster' : ['gbtree'],'gamma' : [0,0.001,0.01],'subsample' : [0.8,0.9,1], 'criterion' : ('mse','mae'), 'max_features' : ('auto', 'sqrt', 'log2')}},\n",
    "    {\"Model_name\" : \"ExtraTree Regressor\", \"Model\" : ExtraTreeRegressor(), \"params\" :  {'max_depth':[4,5,6,10,12,15,20],'n_estimators' : [50,100,150,200]} },\n",
    "    {\"Model_name\" : \"GradientBoosting Regressor\", \"Model\" : GradientBoostingRegressor() , \"params\" : {'max_depth':[4,5,6],'n_estimators' : [50,100,200],'learning_rate' : [0.1,0.2],'subsample' : [0.8,0.9,1]}},\n",
    "    {\"Model_name\" :\"RandomForest Regressor\" , \"Model\" :  RandomForestRegressor(), \"params\" : {'max_depth':[4,5,6,8,10,15],'n_estimators' : [50,100,200]}},\n",
    "    {\"Model_name\" : \"Bagging Regressor\", \"Model\" : BaggingRegressor(), \"params\" : {'n_estimators' : [10,50,100] }}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6EU7t96bNun"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = 3\n",
    "for model_desc in models_description :\n",
    "    print(\"debut\",model_desc[\"Model_name\"])\n",
    "    pre_score = evaluation_model(model_desc[\"Model\"],model_desc[\"Model\"], 3, df_feat,y_full)\n",
    "    model_lat, model_long = gridsearch_multiple_models(df_feat,cv,model_desc[\"params\"],model_desc[\"Model\"])\n",
    "    post_score = evaluation_model(model_lat.best_estimator_,model_long.best_estimator_, 10, df_feat,y_full)\n",
    "    data = dataframe_summary(model_desc[\"Model_name\"],pre_score,post_score,model_long.best_params_,model_lat.best_params_,data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWB4RPfcbNun",
    "outputId": "3234403e-a1fa-4668-a4ac-5fb91e897390"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score_pre_tuning</th>\n",
       "      <th>Score_post_tuning</th>\n",
       "      <th>Hyperparams_lat</th>\n",
       "      <th>Hyperparams_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost Regressor</td>\n",
       "      <td>3.164077</td>\n",
       "      <td>2.634562</td>\n",
       "      <td>{'booster': 'gbtree', 'gamma': 0, 'learning_ra...</td>\n",
       "      <td>{'booster': 'gbtree', 'gamma': 0.001, 'learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTree Regressor</td>\n",
       "      <td>2.875616</td>\n",
       "      <td>2.566386</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 150}</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GradientBoosting Regressor</td>\n",
       "      <td>2.917346</td>\n",
       "      <td>2.708313</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 4, 'n_esti...</td>\n",
       "      <td>{'learning_rate': 0.2, 'max_depth': 4, 'n_esti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForest Regressor</td>\n",
       "      <td>2.892368</td>\n",
       "      <td>2.665724</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 50}</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bagging Regressor</td>\n",
       "      <td>2.959867</td>\n",
       "      <td>2.614221</td>\n",
       "      <td>{'n_estimators': 100}</td>\n",
       "      <td>{'n_estimators': 100}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Score_pre_tuning  Score_post_tuning  \\\n",
       "5           XGBoost Regressor          3.164077           2.634562   \n",
       "6         ExtraTree Regressor          2.875616           2.566386   \n",
       "7  GradientBoosting Regressor          2.917346           2.708313   \n",
       "8      RandomForest Regressor          2.892368           2.665724   \n",
       "9           Bagging Regressor          2.959867           2.614221   \n",
       "\n",
       "                                     Hyperparams_lat  \\\n",
       "5  {'booster': 'gbtree', 'gamma': 0, 'learning_ra...   \n",
       "6             {'max_depth': 15, 'n_estimators': 150}   \n",
       "7  {'learning_rate': 0.1, 'max_depth': 4, 'n_esti...   \n",
       "8               {'max_depth': 8, 'n_estimators': 50}   \n",
       "9                              {'n_estimators': 100}   \n",
       "\n",
       "                                    Hyperparams_long  \n",
       "5  {'booster': 'gbtree', 'gamma': 0.001, 'learnin...  \n",
       "6              {'max_depth': 15, 'n_estimators': 50}  \n",
       "7  {'learning_rate': 0.2, 'max_depth': 4, 'n_esti...  \n",
       "8              {'max_depth': 15, 'n_estimators': 50}  \n",
       "9                              {'n_estimators': 100}  "
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.DataFrame(data).tail(5)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS4G3t50bNuo"
   },
   "source": [
    "**Conclusion :**\n",
    "* On observe une amélioration significative de la prédiction en hypertunant nos modèles\n",
    "* On observe également que les hyperparamètres ne sont pas forcément les mêmes pour les deux outputs (longitude, latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpFME9bobNuo"
   },
   "source": [
    "### Sauvegarde des résultats  \n",
    "Le résultats seront stockés dans un fichier csv pour être utilisé dans d'autres notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yufZg2TbNuo"
   },
   "outputs": [],
   "source": [
    "df_params = df_results[[\"Model\",\"Hyperparams_lat\",\"Hyperparams_long\"]]\n",
    "df_params[\"lng\"] = df_params[\"Hyperparams_long\"]\n",
    "df_params[\"lat\"] = df_params[\"Hyperparams_lat\"]\n",
    "df_params = df_params.drop(columns=[\"Hyperparams_lat\",\"Hyperparams_long\"])\n",
    "df_params.to_csv(\"best_params.csv\",index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbKYkt2BbNuo"
   },
   "outputs": [],
   "source": [
    "## Apercu du fichier sauvegardé\n",
    "df_params = pd.read_csv(\"best_params.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARxmP-L-bNuo"
   },
   "source": [
    "### Récupération des résultats  \n",
    "Une fonction a été ajouté dans notre fichier *IotTools.py* qui retourne les hyperparamètres selon le modèle et le type de coordonnées.  \n",
    "Exemple : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-QREa8HbNuo",
    "outputId": "5eeb28a9-8ffa-4273-8b74-0ecdf8dfb12d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'mae',\n",
       " 'max_depth': 10,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 25}"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hyperparameter(\"RandomForestRegressor\",\"lng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1Sz0IP8bNup"
   },
   "source": [
    "## Aggregation des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKHp7KQMbNup"
   },
   "source": [
    "Afin d'améliorer notre prédiction, on utilisera [VotingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) prenant en compte les résultats de l'ensemble des modèles (en moyennant l'ensemble des prédictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqCNgv0zbNup"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIoYRzaVbNuq"
   },
   "outputs": [],
   "source": [
    "def get_tuned_estimators(coord) : \n",
    "    return [('RandomForestRegressor_lat', RandomForestRegressor(**get_hyperparameter(\"RandomForestRegressor\",coord))),\n",
    "      ('ExtrasTreeRegressor', ExtraTreeRegressor(**get_hyperparameter(\"ExtraTreeRegressor\",coord))),\n",
    "     ('GradientBoostingRegressor', GradientBoostingRegressor(**get_hyperparameter(\"GradientBoostingRegressor\",coord))),\n",
    "     ('XGBRegressor', xgb.XGBRegressor(**get_hyperparameter(\"XGBRegressor\",coord))),\n",
    "     ('BaggingRegressor', BaggingRegressor(**get_hyperparameter(\"BaggingRegressor\",coord)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YASnWOzRbNuq"
   },
   "outputs": [],
   "source": [
    "estimators_lat = get_tuned_estimators(\"lat\")\n",
    "estimators_lng = get_tuned_estimators(\"lng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IzRQ1O84bNuq",
    "outputId": "776c58f8-8162-4227-9a6b-6db7b1169f44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.553526"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_lat = VotingRegressor(estimators=estimators_lat,n_jobs=-1)\n",
    "reg_long = VotingRegressor(estimators=estimators_lng, n_jobs=-1)\n",
    "evaluation_model(reg_lat,reg_long, 10, df_feat,y_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uFxU7HEbNuq"
   },
   "source": [
    "On remarque que cette technique de vote donne un meilleur résultat que les modèles précédents.  \n",
    "\n",
    "Cependant, <u>l'entrainement est assez chronophage</u>    \n",
    "Solution possible : **Sérialiser** le modèle une fois entrainé (en format *.pkl*, par exemple) pour une utilisation ultérieure."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_Tunning model_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
